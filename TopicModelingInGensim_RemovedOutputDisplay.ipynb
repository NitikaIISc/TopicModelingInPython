{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085acf55",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/\n",
    "\n",
    "https://www.youtube.com/watch?v=EZiPAXez4KE\n",
    "\n",
    "https://github.com/prodramp/DeepWorks/blob/main/TopicModelling/BaseWorkshop/Topic_Modeling_Workshop.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a64a9db1",
   "metadata": {},
   "source": [
    "\n",
    "Topic Modeling Workshop Steps:\n",
    "\n",
    "    LDA modeling in Python using the Gensim implementation\n",
    "    Not original work, Get more info in the following README.md\n",
    "        https://github.com/prodramp/DeepWorks/tree/main/TopicModelling/BaseWorkshop\n",
    "\n",
    "Latent Dirichlet Allocation (LDA):\n",
    "\n",
    "    LDA states that each document in a corpus is a combination of a fixed number of topics.\n",
    "    A topic has a probability of generating various words, where the words are all the observed words in the corpus.\n",
    "    These ‘hidden’ topics are then surfaced based on the likelihood of word co-occurrence\n",
    "\n",
    "\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "    Getting the gist of a super large text content\n",
    "    Uncovering the hidden context or structure of a collection of text\n",
    "    User can get an idea about what the given super large collection of text is\n",
    "\n",
    "Steps:\n",
    "\n",
    "    Reading and Loading Data\n",
    "        NIPS Papers as our source dataset\n",
    "    Data Preparation\n",
    "        Select required columns\n",
    "        Remove Punctuations marks\n",
    "    Exploratory Data Analysis\n",
    "        Word Cloud\n",
    "        Document Term Matrix\n",
    "    Data Modeling and tokenization\n",
    "        Stop words removal\n",
    "        bigram and trigram\n",
    "        Vectorization and Tokenization\n",
    "    Model Building\n",
    "        LDA Modelling\n",
    "    Model Evaluations\n",
    "        Model Visualization\n",
    "        Coherance Score\n",
    "\n",
    "Dataset used in this workshop\n",
    "\n",
    "    NIPS papers\n",
    "        https://www.kaggle.com/datasets/benhamner/nips-papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5af48c",
   "metadata": {},
   "source": [
    "## LDA assumes that each document is represented by a distribution of a fixed number of topics, and each topic is a distribution of words.\n",
    "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc374196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbac830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /C:/UCLAAnderson/10KfilingsTables/TopicModeling/papers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62001887",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_location = 'C:/UCLAAnderson/10KfilingsTables/TopicModeling/papers.csv'\n",
    "papers = pd.read_csv(papers_location)\n",
    "papers.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.paper_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67432b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae499b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3e12a",
   "metadata": {},
   "source": [
    "# Trying to get our target data in the same format\n",
    "\n",
    "#### Adding another column called comment_text similar to paper_text to include all the text from html comments as well as multiple pages of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import PyPDF2\n",
    "import io\n",
    "from PyPDF2 import PdfFileReader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata=pd.read_csv('C:/UCLAAnderson/10KfilingsTables/SECClimatePropComments.csv')\n",
    "\n",
    "\n",
    "Metadata.head(25)\n",
    "Metadata['Link to doc']\n",
    "Metadata.iloc[0]\n",
    "\n",
    "\n",
    "for i, j in Metadata.iterrows():\n",
    "#     print(i, j)\n",
    "    print(Metadata['Link to doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install PyCryptodome\n",
    "# import PyCryptodome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac48ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent':'UCLA'}\n",
    "# Metadata[\"comment_text\"] = \"\"\n",
    "Metadata[\"comment_text\"] = np.nan\n",
    "# pd.DataFrame(np.empty(Metadata.shape, dtype = np.str))\n",
    "\n",
    "\n",
    "\n",
    "for eachRow in list(range(0,len(Metadata),1)):\n",
    "   \n",
    "    if '.htm' in Metadata.iloc[eachRow]['Link to doc']:\n",
    "        print(Metadata.iloc[eachRow]['Link to doc'])\n",
    "        #Request the statement file content\n",
    "        content = requests.get(Metadata.iloc[eachRow]['Link to doc'], headers = headers).content #get content of the requested url\n",
    "\n",
    "        # Create a new variable for parsing\n",
    "        report_soup = BeautifulSoup(content, 'html') # In this example the parser is HTML\n",
    "        # Allstatements_report_soup.append(report_soup)\n",
    "        HTMLComment = report_soup.text.strip()\n",
    "        \n",
    "        Metadata.at[eachRow,'comment_text'] = HTMLComment\n",
    "\n",
    "#         Metadata.iloc[eachRow]['comment_text'] = OneComment\n",
    "        \n",
    "    if '.pdf' in Metadata.iloc[eachRow]['Link to doc']:\n",
    "        print(Metadata.iloc[eachRow]['Link to doc'])\n",
    "        url = Metadata.iloc[eachRow]['Link to doc']\n",
    "\n",
    "        r = requests.get(url)\n",
    "        f = io.BytesIO(r.content)\n",
    "\n",
    "        reader = PdfFileReader(f,strict=False)##<<<this strict=False is important\n",
    "#         pdf_reader = PdfReader(input_file, strict=False)\n",
    "        count = reader.numPages\n",
    "        \n",
    "        output=str()\n",
    "        for i in range(count):\n",
    "            page = reader.getPage(i)\n",
    "            if count<1:\n",
    "                output = reader.getPage().extractText().split('\\n')\n",
    "            else:\n",
    "                output += page.extractText()\n",
    "        \n",
    "        PDFComment = output\n",
    "        Metadata.at[eachRow,'comment_text'] = PDFComment\n",
    "#         Metadata.iloc[eachRow]['comment_text'] = OneComment ##chained indexing is unreliable in pandas\n",
    "\n",
    "#### if the PDF is scanned we will not record any text unless tesseract is used\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata\n",
    "list(range(0,len(Metadata),1))\n",
    "Metadata.tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d81ae",
   "metadata": {},
   "source": [
    "#### https://stackoverflow.com/questions/59766591/extracting-text-from-scanned-pdf-without-saving-the-scan-as-a-new-file-image\n",
    "\n",
    "if you do not want to save each scanned pdf on the system to avoid filling up disk space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if Metadata.at[4,'comment_text']=='':\n",
    "\n",
    "    ### trying to comment out to check redundancy:\n",
    "    \n",
    "# if pd.isnull(Metadata['comment_text'].iloc[4]):\n",
    "#     filePath = Metadata.at[4,'Link to doc']\n",
    "#     print(filePath)\n",
    "#     #doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "    \n",
    "#     content = requests.get(Metadata.iloc[eachRow]['Link to doc'], headers = headers).content #get content of the requested url\n",
    "\n",
    "#     # Create a new variable for parsing\n",
    "#     report_soup = BeautifulSoup(content, 'html') # In this example the parser is HTML\n",
    "#     # Allstatements_report_soup.append(report_soup)\n",
    "#     HTMLComment = report_soup.text.strip()\n",
    "#     path, fileName = os.path.split(filePath)\n",
    "#     fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "#     for page_number, page_data in enumerate(doc):\n",
    "#     #     txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "#         txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "#         print(\"Page # {} - {}\".format(str(page_number),txt))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### trying to comment out to check redundancy:\n",
    "\n",
    "# if pd.isnull(Metadata['comment_text'].iloc[4]):\n",
    "#     filePath = Metadata.at[4,'Link to doc']\n",
    "#     doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "\n",
    "# print(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## trying to comment out to check redundancy:\n",
    "\n",
    "# doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "\n",
    "\n",
    "# if pd.isnull(Metadata['comment_text'].iloc[4]):\n",
    "#     filePath = Metadata.at[4,'Link to doc']\n",
    "    \n",
    "#     ###### If no other way than to save the pdf first, you can keep replacing it to avoid filling your disk space though\n",
    "    \n",
    "#     #####https://stackoverflow.com/questions/70863342/how-to-convert-web-pdf-to-text\n",
    "    \n",
    "    \n",
    "#     from urllib.request import urlopen\n",
    "#     import pdfplumber\n",
    "#     url = 'https://archives.nseindia.com/corporate/ICRA_26012022091856_BSER3026012022.pdf'\n",
    "#     response = urlopen(url)\n",
    "#     file = open(\"img.pdf\", 'wb')\n",
    "#     file.write(response.read())\n",
    "#     file.close()\n",
    "#     try:\n",
    "#         pdf = pdfplumber.open('img.pdf')\n",
    "#     except: \n",
    "#         # Some files are not pdf, these are annexes and we don't want them. Or error reading the pdf (damaged ? )\n",
    "#         print(f'Error. Are you sure this is a PDF ?')\n",
    "#         continue\n",
    "        \n",
    "# ############        \n",
    "        \n",
    "# #PDF plumber text extraction\n",
    "# page = pdf.pages[0]\n",
    "# text = page.extract_text()\n",
    "\n",
    "# #requests.get(filePath, headers = headers).content\n",
    "# path, fileName = os.path.split(filePath)\n",
    "# fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "# for page_number, page_data in enumerate(doc):\n",
    "# #     txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "#     txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "#     print(\"Page # {} - {}\".format(str(page_number),txt))\n",
    "\n",
    "# print(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3350320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(filePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952955b2",
   "metadata": {},
   "source": [
    "# https://www.youtube.com/watch?v=bEaxKSQ4Av8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.get(Metadata.iloc[4]['Link to doc'], headers = headers).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.at[4,'Link to doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597af1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.head(25)\n",
    "#Metadata.iloc[4,4]\n",
    "#PDFComment\n",
    "Metadata.to_csv(\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/Feb21_ALL_SECLetterContentExceptScanned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dad727",
   "metadata": {},
   "source": [
    "## Some pdf files were unfortunately scanned and thus PyPDF2 is not able to extract the text from such letters. We need tesseract for Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee2b82",
   "metadata": {},
   "source": [
    "To add to challenges, Tesseract is not compatible with Windows so we need a virtual environment.\n",
    "\n",
    "https://pythonforundergradengineers.com/how-to-install-pytesseract.html\n",
    "\n",
    "Since tesseract is non-Python package needed to use pytesseract, I think the Anaconda distribution of Python and the conda package manager is the way to go.Pytesseract is a Python package that allows you to extract text from images. If you have a picture that has some text in it, pytesseract can pull out the text into a Python program. That's pretty cool. Pytesseract is a wrapper around a program from Google called tesseract. It's tesseract that extracts the text from pictures. Pytesseract is there to help you use tesseract in your Python programs.\n",
    "\n",
    "Tesseract is a command-line application created by Google that can be used to pull text out of pictures. It is an example of an OCR application, which stands for Optical Character Recognition. Which is just a fancy way of saying using a computer to read text. Tesseract is capable of reading text in many different languages. It works best with computer-generated text like text in PDFs or pictures of receipts and invoices. It has a tougher time with images that contain handwritten words.\n",
    "\n",
    "Before we get started with installation, we need to do something else first: create a virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59334f5",
   "metadata": {},
   "source": [
    "C:\\Users\\nisharma\\Anaconda3\\envs\\tesseract\\Library\\bin\\tesseract.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ocrmypdf\n",
    "# !pip install pytesseract\n",
    "# !pip install opencv-python\n",
    "# !pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75631490",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Metadata.index)\n",
    "# 4384+45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ocrmypdf\n",
    "# import opencv\n",
    "import pdf2image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pytesseract.py\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "# pytesseract.pytesseract.tesseract_cmd = 'C:/Users/nisharma/Anaconda3/envs/tesseract/Library/bin/tesseract.exe'\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/nisharma/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "## https://codetoprosper.com/tesseract-ocr-for-windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "for page_number, page_data in enumerate(doc):\n",
    "#     txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "    txt = pytesseract.image_to_string(page_data)#.encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))\n",
    "#     with open('PflugerLetter.txt', 'w') as outfile:\n",
    "#         outfile.write(txt.replace('\\n\\n', '\\n'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec33b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['foo'].isnull(),'foo'] = df['bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f02a3",
   "metadata": {},
   "source": [
    "#### Looking for empty comment_text column to identify docs that were not read because of being scanned PDFs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7777668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG_ScannedPDFs\n",
    "len(Metadata[Metadata[\"comment_text\"].isnull()])\n",
    "Metadata[\"comment_text\"][4]\n",
    "\n",
    "Metadata_copy2=Metadata.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "\n",
    "# Metadata.loc[Metadata['comment_text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4241e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each row, looking in the comment_text column to see if it is empty and needs the help of tesseract to read the text off of a scanned pdf\n",
    "\n",
    "doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "\n",
    "OG_ScannedPDFs=Metadata_copy2[Metadata_copy2[\"comment_text\"].isnull()]## the letters with scanned PDFs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00859432",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(OG_ScannedPDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(OG_ScannedPDFs)\n",
    "# [Metadata['comment_text'].isnull()]\n",
    "\n",
    "# Metadata[Metadata['comment_text'].isnull()]\n",
    "# OG_ScannedPDFs\n",
    "# df=Metadata\n",
    "# df.loc[df['comment_text'].isnull()]\n",
    "# df[df['comment_text'].isnull()]\n",
    "Metadata.head()\n",
    "print(OG_ScannedPDFs)\n",
    "\n",
    "Metadata_copy2\n",
    "Metadata_copy2[Metadata_copy2[\"comment_text\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f519c6",
   "metadata": {},
   "source": [
    "### Replacing OCR text from scanned PDFs to take their place in the dataframe with dates, commenter names, links and comment_text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473fc62",
   "metadata": {},
   "source": [
    "### Replacing with the big metadata instead of only the scanned PDF subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs=OG_ScannedPDFs\n",
    "# eachrow=1\n",
    "# del(eachrow)\n",
    "# Metadata[\"comment_text\"].isnull()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ScannedPDFs.index)\n",
    "\n",
    "ScannedPDFs.comment_text=ScannedPDFs.comment_text.fillna('')\n",
    "# len(Metadata_copy2[Metadata_copy2[\"comment_text\"].notnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c68bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_merged.drop_duplicates(subset=['Date of comment','Name of commenter','Link to doc']))\n",
    "ScannedPDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dedupe=df_merged.drop_duplicates(subset=['Date of comment','Name of commenter','Link to doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d809376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ScannedPDFs = ScannedPDFs.astype({\"comment_text\": 'str'})\n",
    "print()\n",
    "  \n",
    "# lets find out the data \n",
    "# type after changing\n",
    "print(ScannedPDFs.dtypes)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.iloc[6][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1949ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70863342/how-to-convert-web-pdf-to-text\n",
    "##https://stackoverflow.com/questions/70863342/how-to-convert-web-pdf-to-text\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import pdfplumber\n",
    "AllPages=''\n",
    "\n",
    "for eachrow in list(range(0,len(ScannedPDFs.index))):\n",
    "#     if (Metadata[\"comment_text\"].isnull()) is True:\n",
    "    print(ScannedPDFs.iloc[eachrow])\n",
    "    AllPages=''\n",
    "    # url = 'https://archives.nseindia.com/corporate/ICRA_26012022091856_BSER3026012022.pdf'\n",
    "    url=''\n",
    "    url=ScannedPDFs.iloc[eachrow][\"Link to doc\"]\n",
    "    response = urlopen(url)\n",
    "    file = open(\"img.pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "    try:\n",
    "        pdf = pdfplumber.open('img.pdf')\n",
    "    # break\n",
    "    except: \n",
    "        # Some files are not pdf, these are annexes and we don't want them. Or error reading the pdf (damaged ? )\n",
    "        print(f'Error. Are you sure this is a PDF ?')\n",
    "    #     continue\n",
    "    #PDF plumber text extraction\n",
    "\n",
    "    for i in range(0 ,len(pdf.pages)):\n",
    "        page = pdf.pages[i]\n",
    "\n",
    "        text = page.extract_text().replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        AllPages=AllPages+text\n",
    "        print(\"Page # {} - {}\".format(str(page),text))\n",
    "#         print(\"{}\\n\".format(str(page),text))\n",
    "    ScannedPDFs.at[eachrow, 'comment_text'] = AllPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Metadata_copy.iloc[29][\"comment_text\"])\n",
    "type(ScannedPDFs.iloc[0][\"comment_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.iloc[0][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6690e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bed341",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllPages\n",
    "ScannedPDFs.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.to_csv(\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/Feb22_2023ScannedPDFstoTextUsingTesseract.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy=Metadata\n",
    "ScannedPDFs_copy=ScannedPDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fda93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=Metadata_copy[Metadata_copy[\"comment_text\"].notnull()]\n",
    "df2=ScannedPDFs_copy\n",
    "df_merged = df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9281df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.iloc[6][\"comment_text\"]\n",
    "# df_merged[\"comment_text\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged)\n",
    "# Metadata_copy[Metadata_copy[\"comment_text\"].notnull()]\n",
    "print(Metadata[Metadata['comment_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e373256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ScannedPDFs_copy.head(25).colnames())\n",
    "print(Metadata_copy.head(25).columns)\n",
    "print(ScannedPDFs_copy.head(25).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74160e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged.index)\n",
    "# Scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae34e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs_copy=ScannedPDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy=Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38706cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.A.loc[df.A['col'].isna(), 'col'] = df.A.merge(df.B, how='left', on='date')['col_y']\n",
    "# Date of comment \tName of commenter \tLink to doc \t\n",
    "merged=pd.merge(Metadata_copy, ScannedPDFs_copy, on=['Date of comment','Name of commenter','Link to doc'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46895d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ScannedPDFs_copy[\"comment_text\"][1])\n",
    "\n",
    "# Metadata[]\n",
    "print(ScannedPDFs_copy[\"comment_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053267c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy.fillna(ScannedPDFs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6482f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy['comment_text'] = Metadata_copy['comment_text'].apply(str) \n",
    "for eachrow in list(range(0,len(Metadata_copy.index))):\n",
    "    if len(Metadata_copy[\"comment_text\"][eachrow])==0:\n",
    "        print(ScannedPDFs_copy[eachrow][\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b10353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata_copy[\"comment_text\"][4].isnull()\n",
    "type(Metadata[\"comment_text\"][4])\n",
    "# print (Metadata_copy.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674524fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda x: False if pd.isnull(Metadata_copy) else True\n",
    "list(range(0,len(Metadata_copy.index)))\n",
    "ScannedPDFs_copy.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f58daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Metadata_copy[\"comment_text\"][4])\n",
    "\n",
    "df1=Metadata_copy[Metadata_copy.comment_text.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy[Metadata_copy[\"comment_text\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06170f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=ScannedPDFs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a93806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"BothScannedandUnscannedText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99540651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb611e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_copy['comment_text'] = Metadata_copy['comment_text'].apply(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf352a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(eachrow)\n",
    "\n",
    "AllPages=''\n",
    "# url = 'https://archives.nseindia.com/corporate/ICRA_26012022091856_BSER3026012022.pdf'\n",
    "url=''\n",
    "url=ScannedPDFs.iloc[eachrow][\"Link to doc\"]\n",
    "response = urlopen(url)\n",
    "file = open(\"img.pdf\", 'wb')\n",
    "file.write(response.read())\n",
    "file.close()\n",
    "try:\n",
    "    pdf = pdfplumber.open('img.pdf')\n",
    "# break\n",
    "except: \n",
    "    # Some files are not pdf, these are annexes and we don't want them. Or error reading the pdf (damaged ? )\n",
    "    print(f'Error. Are you sure this is a PDF ?')\n",
    "#     continue\n",
    "#PDF plumber text extraction\n",
    "\n",
    "for i in range(0 ,len(pdf.pages)):\n",
    "    page = pdf.pages[i]\n",
    "\n",
    "    text = page.extract_text().replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "    AllPages=AllPages+text\n",
    "    print(\"Page # {} - {}\".format(str(page),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0 ,len(pdf.pages)):\n",
    "    page = pdf.pages[i]\n",
    "\n",
    "    text = page.extract_text().replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "    AllPages=AllPages+text\n",
    "    print(\"Page # {} - {}\".format(str(page),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97376a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.extract_text().replace('\\\\n', '\\n').replace('\\\\t', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7037f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url\n",
    "len(pdf.pages)\n",
    "page\n",
    "# text\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(range(0,len(ScannedPDFs)))))\n",
    "ScannedPDFs.iloc[6][\"comment_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea02d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ScannedPDFs)\n",
    "\n",
    "ScannedPDFs.to_csv('ClimateLetterWritersToSEC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CommentWriters=ScannedPDFs[[\"Date of comment\",\"Name of commenter\"]]\n",
    "CommentWriters.to_csv('ClimateCommenters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.iloc[4]['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1a903",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09cc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:/UCLAAnderson/10KfilingsTables/TopicModeling/Feb21_ALL_SECLetterContentExceptScanned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For simplicity right now, Let's call non scanned data as scanned data\n",
    "ScannedPDFs=Metadata\n",
    "ScannedPDFs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a490a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea8522",
   "metadata": {},
   "source": [
    "## first trying only with scanned PDFs but later replace ScannedPDFs with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(str(ScannedPDFs['comment_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98002706",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs['clean_text'] = ScannedPDFs['comment_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0001ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##cleaning data\n",
    "# docs1=ScannedPDFs\n",
    "# docs=[d.replace(\"See \",\"\") for d in docs1]\n",
    "# docs = [re.sub(r\"\\([^()]*\\)\",\"\",d).replace(\" .\",\".\") for d in docs]\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##cleaning data\n",
    "# docs=[d.replace(\"See \",\"\") for d in docs]\n",
    "# docs = [re.sub(r\"\\([^()]*\\)\",\"\",d).replace(\" .\",\".\") for d in docs]\n",
    "# docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.head(25)\n",
    "ScannedPDFs.iloc[4]['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668d534",
   "metadata": {},
   "source": [
    "#### Removing Digits and all words with digits into it - won't do this because Scope 1 Scope 2 Scope 3 mean somwthing useful for us.\n",
    "##### papers['clean_text'] = papers['clean_text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c72e5",
   "metadata": {},
   "source": [
    "#### Lowercase all text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs['clean_text'] = ScannedPDFs['clean_text'].astype(str).map(lambda x: x.lower())\n",
    "## astype string allows the machine to know that you want this column to be treated as str instead of float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a9c58",
   "metadata": {},
   "source": [
    "### Exploratory Data (Text) analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd394281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def create_word_cloud(target_df, column_name):\n",
    "  print('Joining all words into long text....')\n",
    "  full_text = ','.join(list(target_df[column_name].values))\n",
    "  wordcloud = WordCloud(background_color=\"black\", \n",
    "                        max_words=100,  # top 100 words in the\n",
    "                        contour_width=2, \n",
    "                        contour_color='yellow')\n",
    "  print('Creating word cloud')\n",
    "  wordcloud.generate(full_text)\n",
    "  return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = create_word_cloud(ScannedPDFs, 'clean_text')\n",
    "wordcloud.to_image() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file('March62023_NonScannedOnlyWordcloud.png')## exporting and saving as png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ScannedPDFs['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ec764",
   "metadata": {},
   "source": [
    "#### Document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def create_document_term_matrix(dataframe, column_name):\n",
    "  cv = CountVectorizer(analyzer='word')\n",
    "  data = cv.fit_transform(dataframe[column_name])\n",
    "  df_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names_out())\n",
    "  df_dtm.index=dataframe.index\n",
    "  return df_dtm\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3080004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm = create_document_term_matrix(ScannedPDFs, 'clean_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736abb6",
   "metadata": {},
   "source": [
    "## Data Modeling and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716b7a1",
   "metadata": {},
   "source": [
    "##### \n",
    "4.1. Removing stop words and Lemmatization\n",
    "\n",
    "Remove Stopwords\n",
    "\n",
    "    Text Classification\n",
    "        Spam Filtering\n",
    "        Language Classification\n",
    "        Genre Classification\n",
    "    Caption Generation\n",
    "    Auto-Tag Generation\n",
    "    Topic Modeling\n",
    "\n",
    "Avoid Stopword Removal\n",
    "\n",
    "    Machine Translation\n",
    "    Language Modeling\n",
    "    Text Summarization\n",
    "    Question-Answering problems\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6e4c6",
   "metadata": {},
   "source": [
    "##### 4.1.1 Using gensim NLTK library for removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06156002",
   "metadata": {},
   "source": [
    "### 2.1 Using Spacy library for removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce316029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['has', 'been', 're', 'com', 'edu', 'use','sec','nan','nan ','also','make','disclosure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set deacc=True which will removes the punctuations\n",
    "def convert_sentences_to_words(sentences):\n",
    "    for sentence in sentences:        \n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_stop_words(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d75f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list = ScannedPDFs.clean_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ae41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_words = list(convert_sentences_to_words(text_to_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_as_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "clean_words = remove_all_stop_words(text_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdda2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_words[:1][0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe26e7b",
   "metadata": {},
   "source": [
    "## \n",
    "4.2. Creating Bigram (2 words compound words) and Trigram (3 words compound words)\n",
    "\n",
    "    Bigram\n",
    "        google search\n",
    "        machine learning\n",
    "        artificial intelligence\n",
    "    Trigram\n",
    "        on the table\n",
    "        natural language processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if you will use higher threshold, which will return the fewer phrases.\n",
    "bigram = gensim.models.Phrases(clean_words, min_count=2, threshold=50) \n",
    "trigram = gensim.models.Phrases(bigram[clean_words], threshold=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_bigrams\n",
    "\n",
    "type(clean_words_bigrams)\n",
    "\n",
    "clean_words_bigrams['scope_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://datascience.stackexchange.com/questions/41285/any-efficient-way-to-find-surrounding-adjective-verbs-with-respect-to-the-target\n",
    "\n",
    "#POS-tagging consist of qualifying words by attaching a Part-Of-Speech to it. Part-Of-Speech is a tag that indicates the role of a word in a sentence (e.g. a noun, a transitive verb, a comparative adjective, etc.). You need this to know if a word is an adjective, and it is easily done with the nltk package you are using [source]:\n",
    "# with open('cherrypick_list.txt', 'w') as f:\n",
    "# for k,v  in clean_words_lemmatized.items():\n",
    "#     val = [item if isinstance(item, str) else \" \".join(item) for item in v ]\n",
    "#     f.write(\"{} {}\\n\".format(k,val))\n",
    "# close_words_list=clean_words_lemmatized\n",
    "# >> ('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN')\n",
    "\n",
    "# Here, JJ means \"Adjective\" and \"NN\" means \"Common Noun\".\n",
    "\n",
    "# In your case, you are interested in neighbors adjectives. Does that mean \"the closest adjective\" in the sentence ? Or adjectives within a radius of the target, if any? Depending on the definition, the way to do it differs.\n",
    "\n",
    "# For adjectives within a radius, as you have already selected words within a radius using the snippet you mentioned, you can POS-tag them and then select only those with a tag that indicates an adjective tag.\n",
    "# close_words_list=[int(v) for k,v in clean_words_lemmatized[0].items()]\n",
    "adjective_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "close_adjectives_list = [a[0] for a in nltk.pos_tag(\" \".join(clean_words_lemmatized)) if a[1] in adjective_tags ]\n",
    "\n",
    "# You can look at this answer that list most of existing POS-tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9be61",
   "metadata": {},
   "source": [
    "### https://stackoverflow.com/questions/13547581/python-adjective-synsets-in-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('genesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> finder = BigramCollocationFinder.from_words(\n",
    "...     nltk.corpus.genesis.words('english-web.txt'),\n",
    "...     window_size = 20)\n",
    ">>> finder.apply_freq_filter(2)\n",
    ">>> ignored_words = nltk.corpus.stopwords.words('english')\n",
    ">>> finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    ">>> finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'creature' as a member\n",
    "creature_filter = lambda *w: 'scope' not in w\n",
    "\n",
    "\n",
    "## Bigrams\n",
    "# finder = BigramCollocationFinder.from_words(nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder = BigramCollocationFinder.from_words(word_tokenize(str(text_to_list)))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "\n",
    "\n",
    "# ## Trigrams\n",
    "# finder = TrigramCollocationFinder.from_words(\n",
    "#    nltk.corpus.genesis.words('english-web.txt'))\n",
    "# # only trigrams that appear 3+ times\n",
    "# finder.apply_freq_filter(3)\n",
    "# # only trigrams that contain 'creature'\n",
    "# finder.apply_ngram_filter(creature_filter)\n",
    "# # return the 10 n-grams with the highest PMI\n",
    "# print finder.nbest(trigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966745c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(\"The grand jury\")\n",
    "adjective_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "close_adjectives_list = [a[0] for a in nltk.pos_tag(\" \".join(close_words_list)) if a[1] in adjective_tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebf0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000ff14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbe23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6ac1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedad4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking frequency of Scope 1\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(str(clean_words_bigrams).split())\n",
    "word_filter = lambda w1, w2: \"scope\" not in (w1, w2)\n",
    "finder.apply_ngram_filter(word_filter)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "raw_freq_ranking = finder.nbest(bigram_measures.raw_freq, 10) #top-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "import collections\n",
    "\n",
    "bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(str(clean_words_bigrams))\n",
    "scored = finder.score_ngrams( bgm.likelihood_ratio  )\n",
    "\n",
    "# Group bigrams by first word in bigram.                                        \n",
    "prefix_keys = collections.defaultdict(list)\n",
    "for key, scores in scored:\n",
    "   prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "# Sort keyed bigrams by strongest association.                                  \n",
    "for key in prefix_keys:\n",
    "   prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "print('scope 1', prefix_keys['scope 1'][:5])\n",
    "print('scope 2', prefix_keys['scope 2'][:5])\n",
    "print('scope 3', prefix_keys['scope 3'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168d0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fade7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text = \"this is a foo bar bar black sheep  foo bar bar black sheep foo bar bar black  sheep shep bar bar black sentence\"\n",
    "text = str(text_to_list)\n",
    "Bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(word_tokenize(text_to_list),window_size=20)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.nbest(Bigram_measures.likelihood_ratio, 10)\n",
    "for i in finder.score_ngrams(Bigram_measures.pmi):\n",
    "    print(i)\n",
    "    \n",
    "# Ngrams with 'creature' as a member\n",
    "creature_filter = lambda *w: 'scope' not in w\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(Bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_freq_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb17d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngrams with 'creature' as a member\n",
    "creature_filter = lambda *w: 'scope' not in w\n",
    "\n",
    "\n",
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(clean_words_bigrams,\n",
    "   nltk.corpus.genesis.words('english-web.txt'))\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "\n",
    "\n",
    "## Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(\n",
    "   nltk.corpus.genesis.words('english-web.txt'))\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only trigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print finder.nbest(trigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d308a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code to demonstrate working of\n",
    "# Bigrams Frequency in String\n",
    "# Using Counter() + generator expression\n",
    "from collections import Counter\n",
    "\t\n",
    "# initializing string\n",
    "test_str = str(clean_words_bigrams)\n",
    "\n",
    "# printing original string\n",
    "print(\"The original string is : \" + str(test_str))\n",
    "\n",
    "# Bigrams Frequency in String\n",
    "# Using Counter() + generator expression\n",
    "res = Counter(test_str[idx : idx + 2] for idx in range(len(test_str) - 1))\n",
    "\n",
    "# printing result\n",
    "print(\"The Bigrams Frequency is : \" + str(dict(res)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a44383",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54228921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a009b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):#, 'ADV'\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aabcd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spacy and Loading model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "clean_words_bigrams = make_bigrams(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "clean_words_lemmatized = lemmatization(clean_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB'])#, 'ADV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86272db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "close_adjectives_list = [a[0] for a in clean_words_lemmatized if a[0] in adjective_tags ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "def smart_tokenizer(sentence, target_phrase):\n",
    "    \"\"\"\n",
    "    Tokenize a sentence using a full target phrase.\n",
    "    \"\"\"\n",
    "    tokenizer = MWETokenizer()\n",
    "    target_tuple = tuple(target_phrase.split())\n",
    "    tokenizer.add_mwe(target_tuple)\n",
    "    token_sentence = nltk.pos_tag(tokenizer.tokenize(sentence))#.split()\n",
    "\n",
    "    # The MWETokenizer puts underscores to replace spaces, for some reason\n",
    "    # So just identify what the phrase has been converted to\n",
    "    temp_phrase = target_phrase.replace(' ', '_')\n",
    "    target_index = [i for i, y in enumerate(token_sentence) if y[0] == temp_phrase]\n",
    "    if len(target_index) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        return token_sentence, target_index[0]\n",
    "\n",
    "\n",
    "def search(text_tag, tokenized_sentence, target_index):\n",
    "    \"\"\"\n",
    "    Search for a part of speech (POS) nearest a target phrase of interest.\n",
    "    \"\"\"\n",
    "    for i, entry in enumerate(tokenized_sentence):\n",
    "        # entry[0] is the word; entry[1] is the POS\n",
    "        ahead = target_index + i\n",
    "        behind = target_index - i\n",
    "        try:\n",
    "            if (tokenized_sentence[ahead][1]) == text_tag:\n",
    "                return tokenized_sentence[ahead][0]\n",
    "        except IndexError:\n",
    "            try:\n",
    "                if (tokenized_sentence[behind][1]) == text_tag:\n",
    "                    return tokenized_sentence[behind][0]\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "x, i = smart_tokenizer(sentence=word_tokenize(str(text_to_list)),\n",
    "                       target_phrase='Scope 1')\n",
    "print(search(adjective_tags, x, i))\n",
    "\n",
    "y, j = smart_tokenizer(sentence=word_tokenize(str(text_to_list)),\n",
    "                       target_phrase=\"Scope 2\")\n",
    "print(search(adjective_tags, y, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ddf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_words_lemmatized[:1][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f8186",
   "metadata": {},
   "source": [
    "#### 4.2 Tokenizing the clean and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_words))\n",
    "print(len(clean_words[0]))\n",
    "print(len(clean_words[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f81577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_words_lemmatized))\n",
    "print(len(clean_words_lemmatized[0]))\n",
    "print(len(clean_words_lemmatized[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd91389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(clean_words_lemmatized)\n",
    "\n",
    "# note: If you do not lematized the clean words you can still use the clean words as below\n",
    "# id2word = corpora.Dictionary(clean_words)\n",
    "\n",
    "\n",
    "# Creating Corpus for the clean words\n",
    "texts = clean_words_lemmatized\n",
    "#texts = clean_words\n",
    "\n",
    "\n",
    "# Creating The Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa12c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:1][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d72bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_words_lemmatized))\n",
    "print(len(clean_words_lemmatized[0]))\n",
    "print(len(clean_words_lemmatized[99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbd898",
   "metadata": {},
   "source": [
    "## 5. Training the LDA Model\n",
    "\n",
    "Latent Dirichlet Allocation (LDA):\n",
    "\n",
    "    Unsupervised machine learning, clustering technique\n",
    "    A topic modeling in which words are represented as topics,\n",
    "    Documents are represented as a collection of these word topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiting the total number of topics we are interested from the given tech corpus\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930d542",
   "metadata": {},
   "source": [
    "###  A LDA model with N (= num_topics) topics\n",
    "    Each Topic is a combination of keywords\n",
    "    Each keyword contribute certain weight to the topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gensin to build the LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, \n",
    "                                       id2word=id2word, \n",
    "                                       num_topics=num_topics,\n",
    "                                       )\n",
    "\n",
    "# Please perform the modelling by adding the following options also to check your modelling performance\n",
    "# random_state=100,\n",
    "# chunksize=100,\n",
    "# passes=10,\n",
    "# per_word_topics=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "#pprint(lda_model.print_topics(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17627b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda\n",
    "# to Save the mode;\n",
    "#doc_lda.save('LDAMod_Scanned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa5d5f",
   "metadata": {},
   "source": [
    "### 6. What is the Dominant topic and its percentage contribution in each document\n",
    "\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "\n",
    "This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000fd76",
   "metadata": {},
   "source": [
    "#### Doing the lemmatization again based on: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b49b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(clean_words_lemmatized, min_count=2, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[clean_words_lemmatized], threshold=10)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(clean_words_lemmatized)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474d19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba372f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f03196",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready\n",
    "clean_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=clean_words_lemmatized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73743f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dominant_topic)\n",
    "df_dominant_topic.to_csv('C:/UCLAAnderson/10KfilingsTables/TopicModeling/DominantTopics4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42c9da",
   "metadata": {},
   "source": [
    "## Plot as table and then export PDF:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "df = pd.DataFrame(df_dominant_topic, columns = (\"Document_No\", \"Dominant_Topic\", \"Topic_Perc_Contrib\",\"Keywords\",\"Text\"))\n",
    "\n",
    "#https://stackoverflow.com/questions/32137396/how-do-i-plot-only-a-table-in-matplotlib\n",
    "fig, ax =plt.subplots(figsize=(12,4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "the_table = ax.table(cellText=df.values,colLabels=df.columns,loc='center')\n",
    "\n",
    "# #https://stackoverflow.com/questions/4042192/reduce-left-and-right-margins-in-matplotlib-plot\n",
    "# pp = PdfPages(\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/DominantTopics.pdf\")\n",
    "# pp.savefig(fig, bbox_inches='tight')\n",
    "# pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee61b12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Analyzing the LDA Model\n",
    "\n",
    "    Understanding and interpreting individual topics\n",
    "    Understanding the relationships between the topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(the_table)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f56542",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782604c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join('C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA'+str(num_topics))\n",
    "\n",
    "os.path.join('C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA'+str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2521854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = r\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA/file.txt\"\n",
    "assert os.path.isfile(path)\n",
    "with open(path, \"r\") as f:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e399e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(r'C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA18/ldavis4_prepared_'+str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis4_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis4_prepared, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786598d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis4_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis4_prepared, r'C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA18/ldavis4_prepared_'+str(num_topics)+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis4_prepared = pickle.load(f)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis4_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bbd56",
   "metadata": {},
   "source": [
    "## Calculating Coherence Score:\n",
    "\n",
    "    Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.\n",
    "\n",
    "    It calculates how often two words, and appear together in the corpus\n",
    "\n",
    "    Higher the keywords you will have in your topic the lower the coherence will be\n",
    "\n",
    "    Lower the keywords you will have the coherence could be higher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=clean_words_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# Note: If you have used only clean_words instead of clean_words_lemmatized then use proper words corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LDA Model Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4be312",
   "metadata": {},
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def create_word_cloud(target_df, column_name):\n",
    "  print('Joining all words into long text....')\n",
    "  full_text = ','.join(list(target_df[column_name].values))\n",
    "  wordcloud = WordCloud(background_color=\"black\", \n",
    "                        max_words=100,  # top 100 words in the\n",
    "                        contour_width=2, \n",
    "                        contour_color='yellow')\n",
    "  print('Creating word cloud')\n",
    "  wordcloud.generate(full_text)\n",
    "  return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = create_word_cloud(df_dominant_topic, 'StrText')\n",
    "wordcloud.to_image() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33013989",
   "metadata": {},
   "source": [
    "## 7. The most representative sentence for each topic\n",
    "\n",
    "Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d235047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a66320",
   "metadata": {},
   "source": [
    "## 8. Frequency Distribution of Word Counts in Documents\n",
    "\n",
    "When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a530f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fd89c",
   "metadata": {},
   "source": [
    "## 9. Word Clouds of Top N Keywords in Each Topic\n",
    "\n",
    "Though you’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics I’ve taken here is followed in the subsequent plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67939949",
   "metadata": {},
   "source": [
    "## 10. Word Counts of Topic Keywords\n",
    "\n",
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n",
    "\n",
    "Let’s plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "You want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81655bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86ebb5",
   "metadata": {},
   "source": [
    "## 11. Sentence Chart Colored by Topic\n",
    "\n",
    "Each word in the document is representative of one of the 4 topics. Let’s color each word in the given documents by the topic id it is attributed to.\n",
    "The color of the enclosing rectangle is the topic assigned to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33361cc1",
   "metadata": {},
   "source": [
    "## 12. What are the most discussed topics in the documents?\n",
    "\n",
    "Let’s compute the total number of documents attributed to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bcf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f43692",
   "metadata": {},
   "source": [
    "## Two Plots:\n",
    "### The number of documents for each topic by assigning the document to the topic that has the most weight in that document.\n",
    "### The number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3de43",
   "metadata": {},
   "source": [
    "## 13. t-SNE Clustering Chart\n",
    "\n",
    "Let’s visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f100c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f098d8",
   "metadata": {},
   "source": [
    "### We started from scratch by importing, cleaning and processing the newsgroups dataset to build the LDA model. Then we saw multiple ways to visualize the outputs of topic models including the word clouds and sentence coloring, which intuitively tells you what topic is dominant in each topic. A t-SNE clustering and the pyLDAVis are provide more details into the clustering of the topics.\n",
    "\n",
    "Where next? If you are familiar with scikit learn, you can build and grid search topic models using scikit learn as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8e628",
   "metadata": {},
   "source": [
    "#### https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce825b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ','.join(str(v) for v in df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['StrText'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832112a",
   "metadata": {},
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing punctuations as unnecessary additional computation\n",
    "ScannedPDFs['clean_text'] = ScannedPDFs['comment_text'].apply(lambda x: re.sub('[,\\.!?]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs1.iloc[0][\"comment_text\"] = ScannedPDFs1.iloc[0][\"comment_text\"].fillna(text)\n",
    "ScannedPDFs1.at[0, 'comment_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs1.at[0, 'comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ScannedPDFs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ScannedPDFs.at[1, 'comment_text'])\n",
    "len(ScannedPDFs.index)\n",
    "# AllPages\n",
    "# print(str(text))\n",
    "url\n",
    "ScannedPDFs.iloc[eachrow][\"Link to doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ScannedPDFs.at[0,'comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ScannedPDFs.at[2, 'comment_text'])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs.iloc[0][\"Link to doc\"]\n",
    "\n",
    "# filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "# from pytesseract import image_to_string\n",
    "# from PIL import Image\n",
    "\n",
    "# converted_scan = convert_from_path(filePath, 500)\n",
    "\n",
    "# for i in converted_scan:\n",
    "#     i.save('scan_image.png', 'png')\n",
    "\n",
    "# for page_number, page_data in enumerate(doc):\n",
    "# text = image_to_string(Image.open('scan_image.png'))\n",
    "# with open('scan_text_output.txt', 'w') as outfile:\n",
    "#     outfile.write(text.replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing pdfplumber\n",
    "# conda install -c conda-forge pdfplumber#https://anaconda.org/conda-forge/pdfplumber\n",
    "# ScannedPDFs.iloc[0][\"Link to doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70863342/how-to-convert-web-pdf-to-text\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import pdfplumber\n",
    "# url = 'https://archives.nseindia.com/corporate/ICRA_26012022091856_BSER3026012022.pdf'\n",
    "url=ScannedPDFs.iloc[1][\"Link to doc\"]\n",
    "response = urlopen(url)\n",
    "file = open(\"img.pdf\", 'wb')\n",
    "file.write(response.read())\n",
    "file.close()\n",
    "try:\n",
    "    pdf = pdfplumber.open('img.pdf')\n",
    "# break\n",
    "except: \n",
    "    # Some files are not pdf, these are annexes and we don't want them. Or error reading the pdf (damaged ? )\n",
    "    print(f'Error. Are you sure this is a PDF ?')\n",
    "#     continue\n",
    "#PDF plumber text extraction\n",
    "for i in range(0 ,len(pdf.pages)):\n",
    "    page = pdf.pages[i]\n",
    "    text = page.extract_text()\n",
    "    print(\"Page # {} - {}\".format(str(page),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76dfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for page_number, page_data in enumerate(pdf):\n",
    "#     print(\"Page # {} - {}\".format(str(page_number),text))\n",
    "\n",
    "totalpages = for i in range(0 ,totalpages):\n",
    "    for i in range(0 ,totalpages):\n",
    "        pageobj = pdf.pages[i]\n",
    "        print(pageobj.extract_text())\n",
    "        print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c296ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file_path='C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as Pimage, ImageDraw\n",
    "from wand.image import Image as Wimage\n",
    "import sys\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "import pyocr\n",
    "import pyocr.builders\n",
    "\n",
    "def _convert_pdf2jpg(in_file_path: str, resolution: int=300) -> Pimage:\n",
    "    \"\"\"\n",
    "    Convert PDF file to JPG\n",
    "\n",
    "    :param in_file_path: path of pdf file to convert\n",
    "    :param resolution: resolution with which to read the PDF file\n",
    "    :return: PIL Image\n",
    "    \"\"\"\n",
    "    with Wimage(filename=in_file_path, resolution=resolution).convert(\"jpg\") as all_pages:\n",
    "        for page in all_pages.sequence:\n",
    "            with Wimage(page) as single_page_image:\n",
    "                # transform wand image to bytes in order to transform it into PIL image\n",
    "                yield Pimage.open(BytesIO(bytearray(single_page_image.make_blob(format=\"jpeg\"))))\n",
    "\n",
    "tools = pyocr.get_available_tools()\n",
    "if len(tools) == 0:\n",
    "    print(\"No OCR tool found\")\n",
    "    sys.exit(1)\n",
    "# The tools are returned in the recommended order of usage\n",
    "tool = tools[0]\n",
    "print(\"Will use tool '%s'\" % (tool.get_name()))\n",
    "# Ex: Will use tool 'libtesseract'\n",
    "\n",
    "langs = tool.get_available_languages()\n",
    "print(\"Available languages: %s\" % \", \".join(langs))\n",
    "lang = langs[27]\n",
    "print(\"Will use lang '%s'\" % (lang))\n",
    "# Ex: Will use lang 'fra'\n",
    "# Note that languages are NOT sorted in any way. Please refer\n",
    "# to the system locale settings for the default language\n",
    "# to use.\n",
    "for img in _convert_pdf2jpg(ScannedPDFs.iloc[0][\"Link to doc\"]):\n",
    "    txt = tool.image_to_string(img,\n",
    "                               lang=lang,\n",
    "                               builder=pyocr.builders.TextBuilder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in list(range(0,len(ScannedPDFs),1)):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wand.image import Image as w_img\n",
    "# from PIL import Image as p_img\n",
    "# import pyocr.builders\n",
    "# import regex, pyocr, io\n",
    "\n",
    "# infile = 'my_file.pdf'\n",
    "infile = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "tool = pyocr.get_available_tools()[0]\n",
    "tool = tools[0]\n",
    "req_image = []\n",
    "txt = ''\n",
    "\n",
    "# to convert pdf to img and extract text\n",
    "with w_img(filename = infile, resolution = 200) as scan:\n",
    "    image_png = scan.convert('png')\n",
    "    for i in image_png.sequence:\n",
    "        img_page = w_img(image = i)\n",
    "        req_image.append(img_page.make_blob('png'))\n",
    "    for i in req_image:\n",
    "        content = tool.image_to_string(\n",
    "            p_img.open(io.BytesIO(i)),\n",
    "            lang = tool.get_available_languages()[0],\n",
    "            builder = pyocr.builders.TextBuilder()\n",
    "        )\n",
    "        txt += content\n",
    "\n",
    "# to save the output as a .txt file\n",
    "with open(infile[:-4] + '.txt', 'w') as outfile:\n",
    "    full_txt = regex.sub(r'\\n+', '\\n', txt)\n",
    "    outfile.write(full_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd581dd",
   "metadata": {},
   "source": [
    "## We can now read out text off of a scanned pdf like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846b89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "for page_number, page_data in enumerate(doc):\n",
    "#     txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "    txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2134101",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[0]*k*im.width for i in range(k*im.height)]\n",
    "c_array = np.asarray(c)\n",
    "...\n",
    "ime = Image.fromarray(c_array)\n",
    "ime.save(\"your_file.jpeg\")##https://stackoverflow.com/questions/33658709/convert-an-array-into-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aed5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://betterprogramming.pub/beginners-guide-to-tesseract-ocr-using-python-10ecbb426c3d\n",
    "\n",
    "# conda install -c conda-forge tesserocr\n",
    "\n",
    "# !pip install Pillow\n",
    "from PIL import Imagecolumn=Image.open('image1.png')\n",
    "gray = column.convert('L')\n",
    "blackwhite = gray.point(lambda x: 0 if x < 200 else 255, '1')\n",
    "blackwhite.save(\"image1_bw.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesserocr import PyTessBaseAPIwith PyTessBaseAPI() as api:\n",
    "    api.SetImageFile('sample.jpg')\n",
    "    print(api.GetUTF8Text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tesserocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "##poppler wrapper: https://github.com/Belval/pdf2image\n",
    "# https://blog.alivate.com.au/poppler-windows/\n",
    "filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "doc = convert_from_path(filePath, 500,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "for page_number, page_data in enumerate(doc):\n",
    "    txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/nisharma/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'\n",
    "text_from_image = pytesseract.image_to_string(Image.open('C:/UCLAAnderson/10KfilingsTables/image1.png'))\n",
    "print(text_from_image)\n",
    "\n",
    "##Resources:\n",
    "# https://minhaskamal.github.io/DownGit/#/home?url=https:%2F%2Fgithub.com%2Ftesseract-ocr%2Ftessdata%2Fblob%2F3.04.00%2Feng.traineddata\n",
    "# https://stackoverflow.com/questions/7106012/download-a-single-folder-or-directory-from-a-github-repo\n",
    "# https://github.com/tesseract-ocr/tesseract/issues/271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15967fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "images = convert_from_path(\"C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf\", 500,poppler_path=r'C:\\Users\\nisharma\\Anaconda3\\pkgs\\poppler\\poppler-0.68.0_x86\\poppler-0.68.0\\bin')\n",
    "for i, image in enumerate(images):\n",
    "    fname = 'image'+str(i)+'.png'\n",
    "    image.save(fname, \"PNG\")\n",
    "    txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pytesseract.image_to_string(Image.fromarray(images)).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac51f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_image = pytesseract.image_to_string(Image.open('C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'))\n",
    "print(text_from_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.to_csv('CommentsWithPDFHTMLtexts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b35902",
   "metadata": {},
   "outputs": [],
   "source": [
    "count\n",
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Metadata.iloc[4]['Link to doc'])\n",
    "url = Metadata.iloc[4]['Link to doc']\n",
    "\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "\n",
    "reader = PdfFileReader(f)\n",
    "\n",
    "count = reader.numPages\n",
    "\n",
    "output=str()\n",
    "for i in range(count):\n",
    "    page = reader.getPage(i)\n",
    "    output += page.extractText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "## to make sure our engluish model as well as spacy are downloaded correctly\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Metadata.at[0,'comment_text']\n",
    "test_doc = nlp(test)\n",
    "print(test_doc)\n",
    "# type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c633831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nlp_med = spacy.load(\"en_core_web_md\")## this one has the static vectors saved and it is a larger model\n",
    "doc2 = nlp_med(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bed5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = list(doc2.sents)[0]\n",
    "print(sentence2)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.getPage(0).extractText().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,len(Metadata),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff72cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c453c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# headers = {'user-agent':'UCLA'}\n",
    "# #Request the statement file content\n",
    "# content = requests.get(Metadata[\"Link to doc\"][32], headers = headers).content #get content of the requested url\n",
    "\n",
    "# # Create a new variable for parsing\n",
    "# report_soup = BeautifulSoup(content, 'html') # In this example the parser is HTML\n",
    "# # Allstatements_report_soup.append(report_soup)\n",
    "# report_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4646a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
