{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085acf55",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/\n",
    "\n",
    "https://www.youtube.com/watch?v=EZiPAXez4KE\n",
    "\n",
    "https://github.com/prodramp/DeepWorks/blob/main/TopicModelling/BaseWorkshop/Topic_Modeling_Workshop.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a64a9db1",
   "metadata": {},
   "source": [
    "\n",
    "Topic Modeling Workshop Steps:\n",
    "\n",
    "    LDA modeling in Python using the Gensim implementation\n",
    "    Not original work, Get more info in the following README.md\n",
    "        https://github.com/prodramp/DeepWorks/tree/main/TopicModelling/BaseWorkshop\n",
    "\n",
    "Latent Dirichlet Allocation (LDA):\n",
    "\n",
    "    LDA states that each document in a corpus is a combination of a fixed number of topics.\n",
    "    A topic has a probability of generating various words, where the words are all the observed words in the corpus.\n",
    "    These ‘hidden’ topics are then surfaced based on the likelihood of word co-occurrence\n",
    "\n",
    "\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "    Getting the gist of a super large text content\n",
    "    Uncovering the hidden context or structure of a collection of text\n",
    "    User can get an idea about what the given super large collection of text is\n",
    "\n",
    "Steps:\n",
    "\n",
    "    Reading and Loading Data\n",
    "        NIPS Papers as our source dataset\n",
    "    Data Preparation\n",
    "        Select required columns\n",
    "        Remove Punctuations marks\n",
    "    Exploratory Data Analysis\n",
    "        Word Cloud\n",
    "        Document Term Matrix\n",
    "    Data Modeling and tokenization\n",
    "        Stop words removal\n",
    "        bigram and trigram\n",
    "        Vectorization and Tokenization\n",
    "    Model Building\n",
    "        LDA Modelling\n",
    "    Model Evaluations\n",
    "        Model Visualization\n",
    "        Coherance Score\n",
    "\n",
    "Dataset used in this workshop\n",
    "\n",
    "    NIPS papers\n",
    "        https://www.kaggle.com/datasets/benhamner/nips-papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5af48c",
   "metadata": {},
   "source": [
    "## LDA assumes that each document is represented by a distribution of a fixed number of topics, and each topic is a distribution of words.\n",
    "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1a903",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09cc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata = pd.read_csv('C:/UCLAAnderson/10KfilingsTables/TopicModeling/Feb21_ALL_SECLetterContentExceptScanned.csv')\n",
    "Metadata=pd.read_csv('C:/UCLAAnderson/10KfilingsTables/TopicModeling/Mar13_AllNonScanned_num2wordallbutlastrow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.head()\n",
    "# df=Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f85b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df.loc[df['clean_text'].str.contains('scope 1'), 'clean_text'] = 'scope_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"clean_text\"]= df[\"clean_text\"].replace('scope 1', \"scope_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = {'scope 1':'scope_one', 'scope 2':'scope_two', 'scope 3':'scope_three'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Scopes_clean_text\"]=np.nan\n",
    "\n",
    "for eachRow in list(range(0,len(df),1)):\n",
    "    res = []\n",
    "# for text in text_array:\n",
    "    for key in thesaurus:\n",
    "        text = df.loc[eachRow,'clean_text'].replace(key, thesaurus[key])\n",
    "    res.append(text)\n",
    "    print(res)\n",
    "    df.loc[eachRow,'Scopes_clean_text']=res\n",
    "    \n",
    "    if \"scope 1\" in df.loc[eachRow,'clean_text']:\n",
    "        df.loc[eachRow,'clean_text'] = df.loc[eachRow,'clean_text'].replace('scope 1', 'scope_one')\n",
    "    if \"scope 2\" in df.loc[eachRow,'clean_text']:\n",
    "        df.loc[eachRow,'clean_text'] = df.loc[eachRow,'clean_text'].replace('scope 2', 'scope_two')\n",
    "    if \"scope 3\" in df.loc[eachRow,'clean_text']:\n",
    "        df.loc[eachRow,'clean_text'] = df.loc[eachRow,'clean_text'].replace('scope 3', 'scope_three')\n",
    "    \n",
    "print(df.loc[eachRow,'clean_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e908027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/df_replacedScopes123.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2970481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15506778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/57298367/generate-bigrams-but-only-noun-and-verb-combinations\n",
    "\n",
    "import spacy\n",
    "for eachRow in df.index:\n",
    "    dat = df['clean_text'][eachRow]\n",
    "    wordcloud = create_word_cloud(df.loc[[eachRow]], 'clean_text')\n",
    "    wordcloud.to_image() \n",
    "    wordcloud.to_file('March14_Scope123_'+f'test{eachRow}'+'.png')## exporting and saving as png\n",
    "    text = dat#\"The sleeping cat thought that sitting in the couch resting would be a great idea.\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    for i in range(len(doc)):\n",
    "        j = i+1\n",
    "        if j < len(doc):\n",
    "            if (doc[i].pos_ == \"NOUN\" and doc[j].pos_ == \"ADJ\") or (doc[i].pos_ == \"ADJ\" and doc[j].pos_ == \"NOUN\"):\n",
    "                print(doc[i].text, doc[j].text, doc[i].pos_, doc[j].pos_)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c456d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_clean_text = \"\".join(df['clean_text'].astype('str').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c83465",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937059",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.drop(ScannedPDFs.columns[len(ScannedPDFs.columns)-3], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc116b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a490a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea8522",
   "metadata": {},
   "source": [
    "## first trying only with scanned PDFs but later replace ScannedPDFs with Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80593d9",
   "metadata": {},
   "source": [
    "## Changing numerals to words to retain scope '1','2' &'3' and not get rid of them in bigrams eventually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60627e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = re.sub(r\"(\\d+)\", lambda x: n2w(int(x.group(0))), ScannedPDFs.iloc[3]['comment_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477799c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ScannedPDFs['clean_text'] = ScannedPDFs['clean_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0001ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##cleaning data\n",
    "# docs1=ScannedPDFs\n",
    "# docs=[d.replace(\"See \",\"\") for d in docs1]\n",
    "# docs = [re.sub(r\"\\([^()]*\\)\",\"\",d).replace(\" .\",\".\") for d in docs]\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##cleaning data\n",
    "# docs=[d.replace(\"See \",\"\") for d in docs]\n",
    "# docs = [re.sub(r\"\\([^()]*\\)\",\"\",d).replace(\" .\",\".\") for d in docs]\n",
    "# docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668d534",
   "metadata": {},
   "source": [
    "#### Removing Digits and all words with digits into it - won't do this because Scope 1 Scope 2 Scope 3 mean somwthing useful for us.\n",
    "##### papers['clean_text'] = papers['clean_text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c72e5",
   "metadata": {},
   "source": [
    "#### Lowercase all text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993e886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ScannedPDFs['clean_text'] = ScannedPDFs['clean_text'].astype(str).map(lambda x: x.lower())\n",
    "## astype string allows the machine to know that you want this column to be treated as str instead of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a799b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d70db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.iloc[3]['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b9ad5",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs.to_csv(\"C:/UCLAAnderson/10KfilingsTables/TopicModeling/Mar13_AllNonScanned_num2wordallbutlastrow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98002706",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ee452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ScannedPDFs.loc[5,\"clean_text\"]\n",
    "# # res\n",
    "# res = []\n",
    "# # for text in text_array:\n",
    "# for key in thesaurus:\n",
    "#     text = ScannedPDFs.loc[5,'clean_text'].replace(key, thesaurus[key])\n",
    "# res.append(text)\n",
    "# print(res)\n",
    "# ScannedPDFs.loc[5,'Scopes_clean_text']=res\n",
    "# ScannedPDFs.loc[5,\"Scopes_clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs.loc[3,'clean_text'] = ScannedPDFs.loc[3,'clean_text'].replace('scope 1', 'scope one')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a9c58",
   "metadata": {},
   "source": [
    "### Exploratory Data (Text) analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00458663",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removing Digits and all words with digits into it - won't do this because Scope 1 Scope 2 Scope 3 mean somwthing useful for us.\n",
    "# ScannedPDFs['clean_text'] = ScannedPDFs['clean_text'].apply(lambda x: re.sub('\\w\\d\\w','', x))\n",
    "ScannedPDFs['clean_text'] = ScannedPDFs['clean_text'].apply(lambda x: re.sub(r'\\d+','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs['clean_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd394281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def create_word_cloud(target_df, column_name):\n",
    "  print('Joining all words into long text....')\n",
    "  full_text = ','.join(list(target_df[column_name].values))\n",
    "  wordcloud = WordCloud(background_color=\"black\", \n",
    "                        max_words=100,  # top 100 words in the\n",
    "                        contour_width=2, \n",
    "                        contour_color='yellow')\n",
    "  print('Creating word cloud')\n",
    "  wordcloud.generate(full_text)\n",
    "  return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = create_word_cloud(ScannedPDFs, 'clean_text')\n",
    "wordcloud.to_image() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file('March14_2023_OnlyreplacedScope123_NonScannedOnlyWordcloud.png')## exporting and saving as png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ec764",
   "metadata": {},
   "source": [
    "#### Document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def create_document_term_matrix(dataframe, column_name):\n",
    "  cv = CountVectorizer(analyzer='word')\n",
    "  data = cv.fit_transform(dataframe[column_name])\n",
    "  df_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names_out())\n",
    "  df_dtm.index=dataframe.index\n",
    "  return df_dtm\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3080004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm = create_document_term_matrix(ScannedPDFs, 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736abb6",
   "metadata": {},
   "source": [
    "## Data Modeling and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716b7a1",
   "metadata": {},
   "source": [
    "##### \n",
    "4.1. Removing stop words and Lemmatization\n",
    "\n",
    "Remove Stopwords\n",
    "\n",
    "    Text Classification\n",
    "        Spam Filtering\n",
    "        Language Classification\n",
    "        Genre Classification\n",
    "    Caption Generation\n",
    "    Auto-Tag Generation\n",
    "    Topic Modeling\n",
    "\n",
    "Avoid Stopword Removal\n",
    "\n",
    "    Machine Translation\n",
    "    Language Modeling\n",
    "    Text Summarization\n",
    "    Question-Answering problems\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6e4c6",
   "metadata": {},
   "source": [
    "##### 4.1.1 Using gensim NLTK library for removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06156002",
   "metadata": {},
   "source": [
    "### 2.1 Using Spacy library for removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce316029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['has', 'been', 're', 'com', 'edu', 'use','sec','nan','nan ','also','make','disclosure','gensler','sheila','fowks','dear','chairman','registrant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set deacc=True which will removes the punctuations\n",
    "def convert_sentences_to_words(sentences):\n",
    "    for sentence in sentences:        \n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_stop_words(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d75f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list = ScannedPDFs.clean_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs.clean_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_to_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e672266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num2words = {1: 'One', 2: 'Two', 3: 'Three', 0: 'Zero'}\n",
    "\n",
    "# def n2w(text):\n",
    "#     after_spliting = text.split()\n",
    "\n",
    "#     for index in range(len(after_spliting)):\n",
    "#         if after_spliting[index].isdigit():\n",
    "#             after_spliting[index] = num2words(after_spliting[index])\n",
    "#     numbers_to_words = ' '.join(after_spliting)\n",
    "#     return numbers_to_words\n",
    "# #     try:\n",
    "# #         print(num2words[n])\n",
    "# #     except KeyError:\n",
    "# #         try:\n",
    "# #             print(num2words[n-n%10] + num2words[n%10].lower())\n",
    "# #         except KeyError:\n",
    "# #             print('Number out of range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27365c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n2w('Scope 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ScannedPDFs.iloc[3]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e12815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eff2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inflect\n",
    "# p = inflect.engine()\n",
    "# # p.number_to_words(1234567)\n",
    "# # 'one million, two hundred and thirty-four thousand, five hundred and sixty-seven'\n",
    "# p.number_to_words(Scope 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ae41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_words = list(convert_sentences_to_words(text_to_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352877c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ScannedPDFs.loc[3,'clean_text'])\n",
    "type(text_as_words)\n",
    "text_as_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_as_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "clean_words = remove_all_stop_words(text_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdda2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_words[:1][0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe26e7b",
   "metadata": {},
   "source": [
    "## \n",
    "4.2. Creating Bigram (2 words compound words) and Trigram (3 words compound words)\n",
    "\n",
    "    Bigram\n",
    "        google search\n",
    "        machine learning\n",
    "        artificial intelligence\n",
    "    Trigram\n",
    "        on the table\n",
    "        natural language processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e71b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'creature' as a member\n",
    "scope_filter = lambda *w: 'scope' in w\n",
    "\n",
    "\n",
    "## Bigrams\n",
    "# finder = BigramCollocationFinder.from_words(\n",
    "#    nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder = BigramCollocationFinder.from_words(flatten(clean_words))\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(scope_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "\n",
    "\n",
    "## Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(flatten(clean_words))\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only trigrams that contain 'creature'\n",
    "finder.apply_ngram_filter(scope_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded81e7",
   "metadata": {},
   "source": [
    "## https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-words-in-nlp-f58a93a2f84a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a75ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(flatten(clean_words))\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(flatten(clean_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213636b",
   "metadata": {},
   "source": [
    "Methods to Rank Collocations\n",
    "\n",
    "    Counting frequencies of adjacent words with part of speech filters:\n",
    "\n",
    "The simplest method is to rank the most frequent bigrams or trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c59de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)#trigrams\n",
    "trigram_freq = trigramFinder.ngram_fd.items()\n",
    "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a99e02",
   "metadata": {},
   "source": [
    "#### However, a common issue with this is adjacent spaces, stop words, articles, prepositions or pronouns are common and are not meaningful:\n",
    "To fix this, we filter out for collocations not containing stop words and filter for only the following structures:\n",
    "\n",
    "    Bigrams: (Noun, Noun), (Adjective, Noun)\n",
    "    Trigrams: (Adjective/Noun, Anything, Adjective/Noun)\n",
    "\n",
    "This is a common structure used in literature and generally works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eed5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]#function to filter for trigrams\n",
    "def rightTypesTri(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False#filter trigrams\n",
    "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd315b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tri\n",
    "# type(filtered_bi.iloc[0]['bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bi = [''.join(i) for i in filtered_bi['bigram']]\n",
    "res_tri = [''.join(i) for i in filtered_tri['trigram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6170f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = res_tri\n",
    "r = [s for s in test_list if 'scope' in s]\n",
    "print(r[0] if r else 'nomatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list = (filtered_bi.bigram + ' ').sum().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = filtered_bi['bigram'].str.contains('scope', case=False, na=False)\n",
    "# filtered_bi[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dee972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finder = BigramCollocationFinder.from_words(text.split())\n",
    "# word_filter = lambda w1, w2: \"scope\" in (w1, w2)\n",
    "# bigramFinder.apply_ngram_filter(word_filter)\n",
    "\n",
    "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# raw_freq_ranking = finder.nbest(bigram_measures.raw_freq, 10) #top-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80603a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_freq_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce12cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import Matcher\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# # Add match ID \"orderintake\" with no callback and one pattern\n",
    "# pattern = [{\"LOWER\": \"scope\"}, {\"LOWER\": \"three\"}]\n",
    "# matcher.add(\"scopethree\", [pattern])\n",
    "\n",
    "# # doc = nlp(\"order intake is strong for Q4\")\n",
    "# doc=filtered_bi.bigram\n",
    "# matches = matcher(doc)\n",
    "# print(len(matches)) #Number of times the bi-gram appears in text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc281a25",
   "metadata": {},
   "source": [
    "## 2. Pointwise Mutual Information\n",
    "\n",
    "The Pointwise Mutual Information (PMI) score for bigrams is:\n",
    "\n",
    "For trigrams:\n",
    "\n",
    "The main intuition is that it measures how much more likely the words co-occur than if they were independent. However, it is very sensitive to rare combination of words. For example, if a random bigram ‘abc xyz’ appears, and neither ‘abc’ nor ‘xyz’ appeared anywhere else in the text, ‘abc xyz’ will be identified as highly significant bigram when it could just be a random misspelling or a phrase too rare to generalize as a bigram. Therefore, this method is often used with a frequency filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only those with more than 20 occurences\n",
    "bigramFinder.apply_freq_filter(20)\n",
    "trigramFinder.apply_freq_filter(20)\n",
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b92cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ee124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigramPMITable[bigramPMITable['bigram'].str.contains(\"scope\")]\n",
    "\n",
    "res2 = [''.join(i) for i in trigramPMITable['trigram']]\n",
    "test_list = res2\n",
    "r = [s for s in test_list if s.startswith('scope')]\n",
    "print(r[0] if r else 'nomatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6845c24",
   "metadata": {},
   "source": [
    "##### We can see that PMI picks up bigrams and trigrams that consist of words that should co-occur together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea072d",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing\n",
    "\n",
    "### a. t-test:\n",
    "\n",
    "Consider if we have a corpus with N words, and social and media have word counts C(social) and C(media) respectively. Assuming null hypothesis with social and media being independent:\n",
    "\n",
    "The test statistic is:\n",
    "\n",
    "However, the same problem occurs where pairs with prepositions, pronouns, articles etc. come up as most significant. Therefore, we need to apply the same filters from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n",
    "trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','t']).sort_values(by='t', ascending=False)#filters\n",
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n",
    "filteredT_tri = trigramTtable[trigramTtable.trigram.map(lambda x: rightTypesTri(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = [''.join(i) for i in filteredT_tri['trigram']]\n",
    "test_list = res3\n",
    "r = [s for s in test_list if s.startswith('scope')]\n",
    "print(r[0] if r else 'nomatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f49b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredT_bi\n",
    "res3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f66c5c",
   "metadata": {},
   "source": [
    "##### T-test has been criticized as it assumes normal distribution. Therefore, we will also look into the chi-square test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedf9a3",
   "metadata": {},
   "source": [
    "### b. chi-square test\n",
    "\n",
    "First, we compute a table like below for each word pair:\n",
    "\n",
    "The chi-square test assumes in the null hypothesis that words are independent, just like in t-test. The chi-square test statistic is computed as:\n",
    "\n",
    "Results are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c69903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebee4a1",
   "metadata": {},
   "source": [
    "We can see that PMI and chi-square methods give pretty good results even without applying filters. Their results are also quite similar. Frequency and T-test methods are also similar to each other. In real applications, we can eyeball the list and set a threshold at a value from when the list stops making sense. We can also do different tests to see which list seems to make the most sense for a given dataset. Alternatively, we can combine results from multiple lists. Personally, I find it effective to multiply PMI and frequency to take into account both probability lift and frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredT_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cde15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7efba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ee74dff",
   "metadata": {},
   "source": [
    "############################################################\n",
    "# Old successful method\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf21d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if you will use higher threshold, which will return the fewer phrases.\n",
    "bigram = gensim.models.Phrases(clean_words, min_count=1, threshold=300) \n",
    "trigram = gensim.models.Phrases(bigram[clean_words], threshold=150)  \n",
    "# quadgram = gensim.models.Phrases(trigram[clean_words], threshold=150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# quadgram_mod = gensim.models.phrases.Phraser(quadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59818f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def make_quadgrams(texts):\n",
    "#     return [quadgram_mod[trigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5febe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spacy and Loading model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54297749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "clean_words_bigrams = make_bigrams(clean_words)\n",
    "\n",
    "clean_words_trigrams = make_trigrams(clean_words)\n",
    "# clean_words_trigrams=res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_trigrams\n",
    "# [filteredT_tri['trigram']]\n",
    "# clean_words_trigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc46c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_old = [''.join(i) for i in clean_words_trigrams]\n",
    "# test_list = res_old\n",
    "# r = [s for s in test_list if 'scope' in s]\n",
    "# print(r[0] if r else 'nomatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_words_trigrams = make_trigrams(clean_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_words_quadgrams = make_quadgrams(clean_words_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(clean_words_bigrams[3])\n",
    "xs=clean_words_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58385478",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(\"scope\" in s for s in xs):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129101b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [s for s in xs if \"scope\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e064601",
   "metadata": {},
   "source": [
    "## To avoid:\n",
    "ValueError: [E088] Text of length 1341607 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f28d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2970481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_trigrams\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a469c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "clean_words_lemmatized = lemmatization(clean_words, allowed_postags=['NOUN', 'ADJ','VERB'])##'ADV', 'VERB',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dec272",
   "metadata": {},
   "source": [
    "## Not useful to do lemmatization after creation of trigrams as breaks them down to letters instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4906d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(flatten(clean_words_lemmatized))\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(flatten(clean_words_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56701460",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
    "trigramChiTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','chi-sq']).sort_values(by='chi-sq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_words_lemmatized[:1][0][:100])\n",
    "trigramChiTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0dd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n",
    "trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','t']).sort_values(by='t', ascending=False)#filters\n",
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n",
    "filteredT_tri = trigramTtable[trigramTtable.trigram.map(lambda x: rightTypesTri(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f16546",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredT_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3a = [''.join(i) for i in filteredT_bi['bigram']]\n",
    "res3b = [''.join(i) for i in filteredT_tri['trigram']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05006c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs2=res3a\n",
    "xs3=res3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3a\n",
    "res3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized2 = lemmatization(filteredT_bi['bigram'], allowed_postags=['NOUN', 'ADJ'])##'ADV', 'VERB',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredT_tri['trigram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de867e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fe596",
   "metadata": {},
   "source": [
    "### Find adjective and noun comparisons\n",
    "https://stackoverflow.com/questions/49166071/how-to-extract-noun-adjective-pairs-from-a-sentence\n",
    "\n",
    "https://uc-r.github.io/creating-text-features\n",
    "\n",
    "https://towardsdatascience.com/text-exploration-with-python-cb8ea710e07c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# doc = nlp(u'Mark and John are sincere employees at Google.')\n",
    "# doc=nlp(ScannedPDFs['worded_text'][3])\n",
    "doc=nlp(ScannedPDFs['worded_text'][3])\n",
    "noun_adj_pairs = []\n",
    "for i,token in enumerate(doc):\n",
    "    if token.pos_ not in ('NOUN','PROPN'):\n",
    "        continue\n",
    "    for j in range(i+1,len(doc)):\n",
    "        if doc[j].pos_ == 'ADJ':\n",
    "            noun_adj_pairs.append((token,doc[j]))\n",
    "            break\n",
    "noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d777b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259568aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/57298367/generate-bigrams-but-only-noun-and-verb-combinations\n",
    "\n",
    "import spacy\n",
    "\n",
    "text = text_to_list#\"The sleeping cat thought that sitting in the couch resting would be a great idea.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for i in range(len(doc)):\n",
    "    j = i+1\n",
    "    if j < len(doc):\n",
    "        if (doc[i].pos_ == \"NOUN\" and doc[j].pos_ == \"ADJ\") or (doc[i].pos_ == \"ADJ\" and doc[j].pos_ == \"NOUN\"):\n",
    "            print(doc[i].text, doc[j].text, doc[i].pos_, doc[j].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566be06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [s for s in xs2 if \"scopethree\" in s or \"scopetwo\" in s or \"scopeone\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d109ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ad992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_words))\n",
    "print(len(clean_words[0]))\n",
    "print(len(clean_words[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((clean_words_lemmatized))\n",
    "print(len(clean_words_lemmatized[0]))\n",
    "print(len(clean_words_lemmatized[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ec41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3b\n",
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e55a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(clean_words_lemmatized)\n",
    "\n",
    "# note: If you do not lematized the clean words you can still use the clean words as below\n",
    "# id2word = corpora.Dictionary(clean_words)\n",
    "\n",
    "\n",
    "# Creating Corpus for the clean words\n",
    "# texts = clean_words_lemmatized\n",
    "#texts = clean_words\n",
    "texts=clean_words_lemmatized#[res3b]\n",
    "\n",
    "# Creating The Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d807e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:1][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_words_lemmatized))\n",
    "print(len(clean_words_lemmatized[0]))\n",
    "print(len(clean_words_lemmatized[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiting the total number of topics we are interested from the given tech corpus\n",
    "num_topics = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9835d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gensin to build the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "\n",
    "# lda_model = gensim.models.LdaMulticore(corpus=corpus, \n",
    "#                                        id2word=id2word, \n",
    "#                                        num_topics=num_topics,\n",
    "#                                        )\n",
    "\n",
    "# Please perform the modelling by adding the following options also to check your modelling performance\n",
    "# random_state=100,\n",
    "# chunksize=100,\n",
    "# passes=10,\n",
    "# per_word_topics=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the N (=num_topics) topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "#pprint(lda_model.print_topics(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5af991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac66390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join('C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA18/March15_15topicsScope123Bigram_Learn_AllScannum2word_ldavis_prepared_'+str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7754f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, 'C:/UCLAAnderson/10KfilingsTables/TopicModeling/LDA18/March15_15topicsScope123Bigram_ldavis_prepared_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e014de",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71007d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=clean_words_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# Note: If you have used only clean_words instead of clean_words_lemmatized then use proper words corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6264d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LDA Model Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa03a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282af96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = []\n",
    "for k in range(5,25):\n",
    "    print('Round: '+str(k))\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=k, \\\n",
    "               id2word = dictionary, passes=40,\\\n",
    "               iterations=200, chunksize = 10000, eval_every = None)\n",
    "    \n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
    "         model=ldamodel, texts=final_reviews,\\\n",
    "         dictionary=dictionary, coherence='c_v')   \n",
    "                                                \n",
    "    coherence.append((k,cm.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba36cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c4be312",
   "metadata": {},
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(clean_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[clean_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(clean_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43178782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def create_word_cloud(target_df, column_name):\n",
    "  print('Joining all words into long text....')\n",
    "  full_text = ','.join(list(target_df[column_name].values))\n",
    "  wordcloud = WordCloud(background_color=\"black\", \n",
    "                        max_words=100,  # top 100 words in the\n",
    "                        contour_width=2, \n",
    "                        contour_color='yellow')\n",
    "  print('Creating word cloud')\n",
    "  wordcloud.generate(full_text)\n",
    "  return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = create_word_cloud(df_dominant_topic, 'StrText')\n",
    "wordcloud.to_image() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33013989",
   "metadata": {},
   "source": [
    "## 7. The most representative sentence for each topic\n",
    "\n",
    "Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d235047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a66320",
   "metadata": {},
   "source": [
    "## 8. Frequency Distribution of Word Counts in Documents\n",
    "\n",
    "When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a530f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fd89c",
   "metadata": {},
   "source": [
    "## 9. Word Clouds of Top N Keywords in Each Topic\n",
    "\n",
    "Though you’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics I’ve taken here is followed in the subsequent plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67939949",
   "metadata": {},
   "source": [
    "## 10. Word Counts of Topic Keywords\n",
    "\n",
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n",
    "\n",
    "Let’s plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "You want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81655bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86ebb5",
   "metadata": {},
   "source": [
    "## 11. Sentence Chart Colored by Topic\n",
    "\n",
    "Each word in the document is representative of one of the 4 topics. Let’s color each word in the given documents by the topic id it is attributed to.\n",
    "The color of the enclosing rectangle is the topic assigned to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33361cc1",
   "metadata": {},
   "source": [
    "## 12. What are the most discussed topics in the documents?\n",
    "\n",
    "Let’s compute the total number of documents attributed to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bcf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f43692",
   "metadata": {},
   "source": [
    "## Two Plots:\n",
    "### The number of documents for each topic by assigning the document to the topic that has the most weight in that document.\n",
    "### The number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3de43",
   "metadata": {},
   "source": [
    "## 13. t-SNE Clustering Chart\n",
    "\n",
    "Let’s visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f100c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f098d8",
   "metadata": {},
   "source": [
    "### We started from scratch by importing, cleaning and processing the newsgroups dataset to build the LDA model. Then we saw multiple ways to visualize the outputs of topic models including the word clouds and sentence coloring, which intuitively tells you what topic is dominant in each topic. A t-SNE clustering and the pyLDAVis are provide more details into the clustering of the topics.\n",
    "\n",
    "Where next? If you are familiar with scikit learn, you can build and grid search topic models using scikit learn as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8e628",
   "metadata": {},
   "source": [
    "#### https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce825b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ','.join(str(v) for v in df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['StrText'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_dominant_topic['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832112a",
   "metadata": {},
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing punctuations as unnecessary additional computation\n",
    "ScannedPDFs['clean_text'] = ScannedPDFs['comment_text'].apply(lambda x: re.sub('[,\\.!?]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs1.iloc[0][\"comment_text\"] = ScannedPDFs1.iloc[0][\"comment_text\"].fillna(text)\n",
    "ScannedPDFs1.at[0, 'comment_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScannedPDFs1.at[0, 'comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ScannedPDFs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ScannedPDFs.at[1, 'comment_text'])\n",
    "len(ScannedPDFs.index)\n",
    "# AllPages\n",
    "# print(str(text))\n",
    "url\n",
    "ScannedPDFs.iloc[eachrow][\"Link to doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ScannedPDFs.at[0,'comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ScannedPDFs.at[2, 'comment_text'])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScannedPDFs.iloc[0][\"Link to doc\"]\n",
    "\n",
    "# filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "# from pytesseract import image_to_string\n",
    "# from PIL import Image\n",
    "\n",
    "# converted_scan = convert_from_path(filePath, 500)\n",
    "\n",
    "# for i in converted_scan:\n",
    "#     i.save('scan_image.png', 'png')\n",
    "\n",
    "# for page_number, page_data in enumerate(doc):\n",
    "# text = image_to_string(Image.open('scan_image.png'))\n",
    "# with open('scan_text_output.txt', 'w') as outfile:\n",
    "#     outfile.write(text.replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing pdfplumber\n",
    "# conda install -c conda-forge pdfplumber#https://anaconda.org/conda-forge/pdfplumber\n",
    "# ScannedPDFs.iloc[0][\"Link to doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70863342/how-to-convert-web-pdf-to-text\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import pdfplumber\n",
    "# url = 'https://archives.nseindia.com/corporate/ICRA_26012022091856_BSER3026012022.pdf'\n",
    "url=ScannedPDFs.iloc[1][\"Link to doc\"]\n",
    "response = urlopen(url)\n",
    "file = open(\"img.pdf\", 'wb')\n",
    "file.write(response.read())\n",
    "file.close()\n",
    "try:\n",
    "    pdf = pdfplumber.open('img.pdf')\n",
    "# break\n",
    "except: \n",
    "    # Some files are not pdf, these are annexes and we don't want them. Or error reading the pdf (damaged ? )\n",
    "    print(f'Error. Are you sure this is a PDF ?')\n",
    "#     continue\n",
    "#PDF plumber text extraction\n",
    "for i in range(0 ,len(pdf.pages)):\n",
    "    page = pdf.pages[i]\n",
    "    text = page.extract_text()\n",
    "    print(\"Page # {} - {}\".format(str(page),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76dfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for page_number, page_data in enumerate(pdf):\n",
    "#     print(\"Page # {} - {}\".format(str(page_number),text))\n",
    "\n",
    "totalpages = for i in range(0 ,totalpages):\n",
    "    for i in range(0 ,totalpages):\n",
    "        pageobj = pdf.pages[i]\n",
    "        print(pageobj.extract_text())\n",
    "        print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c296ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file_path='C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as Pimage, ImageDraw\n",
    "from wand.image import Image as Wimage\n",
    "import sys\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "import pyocr\n",
    "import pyocr.builders\n",
    "\n",
    "def _convert_pdf2jpg(in_file_path: str, resolution: int=300) -> Pimage:\n",
    "    \"\"\"\n",
    "    Convert PDF file to JPG\n",
    "\n",
    "    :param in_file_path: path of pdf file to convert\n",
    "    :param resolution: resolution with which to read the PDF file\n",
    "    :return: PIL Image\n",
    "    \"\"\"\n",
    "    with Wimage(filename=in_file_path, resolution=resolution).convert(\"jpg\") as all_pages:\n",
    "        for page in all_pages.sequence:\n",
    "            with Wimage(page) as single_page_image:\n",
    "                # transform wand image to bytes in order to transform it into PIL image\n",
    "                yield Pimage.open(BytesIO(bytearray(single_page_image.make_blob(format=\"jpeg\"))))\n",
    "\n",
    "tools = pyocr.get_available_tools()\n",
    "if len(tools) == 0:\n",
    "    print(\"No OCR tool found\")\n",
    "    sys.exit(1)\n",
    "# The tools are returned in the recommended order of usage\n",
    "tool = tools[0]\n",
    "print(\"Will use tool '%s'\" % (tool.get_name()))\n",
    "# Ex: Will use tool 'libtesseract'\n",
    "\n",
    "langs = tool.get_available_languages()\n",
    "print(\"Available languages: %s\" % \", \".join(langs))\n",
    "lang = langs[27]\n",
    "print(\"Will use lang '%s'\" % (lang))\n",
    "# Ex: Will use lang 'fra'\n",
    "# Note that languages are NOT sorted in any way. Please refer\n",
    "# to the system locale settings for the default language\n",
    "# to use.\n",
    "for img in _convert_pdf2jpg(ScannedPDFs.iloc[0][\"Link to doc\"]):\n",
    "    txt = tool.image_to_string(img,\n",
    "                               lang=lang,\n",
    "                               builder=pyocr.builders.TextBuilder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in list(range(0,len(ScannedPDFs),1)):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wand.image import Image as w_img\n",
    "# from PIL import Image as p_img\n",
    "# import pyocr.builders\n",
    "# import regex, pyocr, io\n",
    "\n",
    "# infile = 'my_file.pdf'\n",
    "infile = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "tool = pyocr.get_available_tools()[0]\n",
    "tool = tools[0]\n",
    "req_image = []\n",
    "txt = ''\n",
    "\n",
    "# to convert pdf to img and extract text\n",
    "with w_img(filename = infile, resolution = 200) as scan:\n",
    "    image_png = scan.convert('png')\n",
    "    for i in image_png.sequence:\n",
    "        img_page = w_img(image = i)\n",
    "        req_image.append(img_page.make_blob('png'))\n",
    "    for i in req_image:\n",
    "        content = tool.image_to_string(\n",
    "            p_img.open(io.BytesIO(i)),\n",
    "            lang = tool.get_available_languages()[0],\n",
    "            builder = pyocr.builders.TextBuilder()\n",
    "        )\n",
    "        txt += content\n",
    "\n",
    "# to save the output as a .txt file\n",
    "with open(infile[:-4] + '.txt', 'w') as outfile:\n",
    "    full_txt = regex.sub(r'\\n+', '\\n', txt)\n",
    "    outfile.write(full_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd581dd",
   "metadata": {},
   "source": [
    "## We can now read out text off of a scanned pdf like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846b89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "doc = convert_from_path(filePath,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "for page_number, page_data in enumerate(doc):\n",
    "#     txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "    txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2134101",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[0]*k*im.width for i in range(k*im.height)]\n",
    "c_array = np.asarray(c)\n",
    "...\n",
    "ime = Image.fromarray(c_array)\n",
    "ime.save(\"your_file.jpeg\")##https://stackoverflow.com/questions/33658709/convert-an-array-into-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aed5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://betterprogramming.pub/beginners-guide-to-tesseract-ocr-using-python-10ecbb426c3d\n",
    "\n",
    "# conda install -c conda-forge tesserocr\n",
    "\n",
    "# !pip install Pillow\n",
    "from PIL import Imagecolumn=Image.open('image1.png')\n",
    "gray = column.convert('L')\n",
    "blackwhite = gray.point(lambda x: 0 if x < 200 else 255, '1')\n",
    "blackwhite.save(\"image1_bw.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesserocr import PyTessBaseAPIwith PyTessBaseAPI() as api:\n",
    "    api.SetImageFile('sample.jpg')\n",
    "    print(api.GetUTF8Text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tesserocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "##poppler wrapper: https://github.com/Belval/pdf2image\n",
    "# https://blog.alivate.com.au/poppler-windows/\n",
    "filePath = 'C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'\n",
    "doc = convert_from_path(filePath, 500,poppler_path='C:/Users/nisharma/Anaconda3/pkgs/poppler/poppler-0.68.0_x86/poppler-0.68.0/bin')\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)\n",
    "\n",
    "for page_number, page_data in enumerate(doc):\n",
    "    txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/nisharma/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'\n",
    "text_from_image = pytesseract.image_to_string(Image.open('C:/UCLAAnderson/10KfilingsTables/image1.png'))\n",
    "print(text_from_image)\n",
    "\n",
    "##Resources:\n",
    "# https://minhaskamal.github.io/DownGit/#/home?url=https:%2F%2Fgithub.com%2Ftesseract-ocr%2Ftessdata%2Fblob%2F3.04.00%2Feng.traineddata\n",
    "# https://stackoverflow.com/questions/7106012/download-a-single-folder-or-directory-from-a-github-repo\n",
    "# https://github.com/tesseract-ocr/tesseract/issues/271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15967fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "images = convert_from_path(\"C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf\", 500,poppler_path=r'C:\\Users\\nisharma\\Anaconda3\\pkgs\\poppler\\poppler-0.68.0_x86\\poppler-0.68.0\\bin')\n",
    "for i, image in enumerate(images):\n",
    "    fname = 'image'+str(i)+'.png'\n",
    "    image.save(fname, \"PNG\")\n",
    "    txt = pytesseract.image_to_string(Image.fromarray(page_data)).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pytesseract.image_to_string(Image.fromarray(images)).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac51f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_image = pytesseract.image_to_string(Image.open('C:/UCLAAnderson/10KfilingsTables/pdfimage.pdf'))\n",
    "print(text_from_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata.to_csv('CommentsWithPDFHTMLtexts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b35902",
   "metadata": {},
   "outputs": [],
   "source": [
    "count\n",
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Metadata.iloc[4]['Link to doc'])\n",
    "url = Metadata.iloc[4]['Link to doc']\n",
    "\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "\n",
    "reader = PdfFileReader(f)\n",
    "\n",
    "count = reader.numPages\n",
    "\n",
    "output=str()\n",
    "for i in range(count):\n",
    "    page = reader.getPage(i)\n",
    "    output += page.extractText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "## to make sure our engluish model as well as spacy are downloaded correctly\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Metadata.at[0,'comment_text']\n",
    "test_doc = nlp(test)\n",
    "print(test_doc)\n",
    "# type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c633831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nlp_med = spacy.load(\"en_core_web_md\")## this one has the static vectors saved and it is a larger model\n",
    "doc2 = nlp_med(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bed5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = list(doc2.sents)[0]\n",
    "print(sentence2)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.getPage(0).extractText().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,len(Metadata),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff72cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c453c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# headers = {'user-agent':'UCLA'}\n",
    "# #Request the statement file content\n",
    "# content = requests.get(Metadata[\"Link to doc\"][32], headers = headers).content #get content of the requested url\n",
    "\n",
    "# # Create a new variable for parsing\n",
    "# report_soup = BeautifulSoup(content, 'html') # In this example the parser is HTML\n",
    "# # Allstatements_report_soup.append(report_soup)\n",
    "# report_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4646a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
